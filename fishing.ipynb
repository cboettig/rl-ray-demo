{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "import gym_conservation\n",
    "import gym_fishing\n",
    "from ray import tune\n",
    "from ray.rllib import agents\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"RLLIB_NUM_GPUS\"] = str(torch.cuda.device_count())\n",
    "\n",
    "## Possible bug, as --shm-size is already large!\n",
    "os.environ[\"RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE\"] = \"1\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## rllib ignores gym registered names, need to register manually:\n",
    "## note these envs were not written to take a single parameter dictionary (\"config\")\n",
    "tune.register_env(\"conservation-v6\", lambda config: gym_conservation.envs.NonStationaryV6())\n",
    "tune.register_env(\"conservation-v5\", lambda config: gym_conservation.envs.NonStationaryV5())\n",
    "tune.register_env(\"fishing-v0\", lambda config: gym_fishing.envs.FishingEnv())\n",
    "tune.register_env(\"fishing-v1\", lambda config: gym_fishing.envs.FishingCtsEnv())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1706583)\u001b[0m 2022-02-22 20:03:02,386\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1706584)\u001b[0m 2022-02-22 20:03:02,825\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "trainer = agents.ppo.PPOTrainer(env=\"fishing-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 20:03:08,139\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1707424)\u001b[0m 2022-02-22 20:03:08,101\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "# Customized configure for the algorithm.\n",
    "config = {\n",
    "    \"env\": \"fishing-v1\",\n",
    "    # Use 2 environment workers (aka \"rollout workers\") that parallelly\n",
    "    # collect samples from their own environment clone(s).\n",
    "    \"num_workers\": 4,\n",
    "    \"framework\": \"torch\",\n",
    "    \"num_gpus\": torch.cuda.device_count(),\n",
    "\n",
    "    \"model\": {\n",
    "        \"fcnet_hiddens\": [64, 64],\n",
    "        \"fcnet_activation\": \"relu\",\n",
    "    },\n",
    "    \"evaluation_num_workers\": 1,\n",
    "    \"evaluation_interval\": 2,\n",
    "    # Only for evaluation runs, render the env.\n",
    "    \"evaluation_config\": {\n",
    "        \"render_env\": False,\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create our RLlib Trainer.\n",
    "trainer = agents.ppo.PPOTrainer(config=config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1707425)\u001b[0m 2022-02-22 20:03:08,488\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1707974)\u001b[0m 2022-02-22 20:03:13,082\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1707975)\u001b[0m 2022-02-22 20:03:13,653\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +12m27s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n",
      "\u001b[2m\u001b[1m\u001b[33m(scheduler +12m27s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-22 20:03:29,664\tWARNING worker.py:1257 -- The actor or task with ID ffffffffffffffffbb8fcfee898bcd16ce826c7201000000 cannot be scheduled right now. You can ignore this message if this Ray cluster is expected to auto-scale or if you specified a runtime_env for this actor or task, which may take time to install.  Otherwise, this is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increasing the resources available to this Ray cluster.\n",
      "Required resources for this actor or task: {CPU: 1.000000}\n",
      "Available resources on this node: {0.000000/24.000000 CPU, 35.478617 GiB/35.478617 GiB memory, 1.000000/1.000000 GPU, 10.999989 GiB/10.999989 GiB object_store_memory, 1.000000/1.000000 accelerator_type:G, 1.000000/1.000000 node:172.18.0.5}\n",
      " In total there are 0 pending tasks and 1 pending actors on this node.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1710793)\u001b[0m 2022-02-22 20:03:35,670\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "for _ in range(4):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': 1.1701428891871886,\n",
       "  'episode_reward_min': 0.75,\n",
       "  'episode_reward_mean': 0.8909582040576767,\n",
       "  'episode_len_mean': 3.4,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 10,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [1.1701428891871886,\n",
       "    1.0337219210797386,\n",
       "    0.8812072934789418,\n",
       "    0.9237330803724098,\n",
       "    0.75,\n",
       "    0.75,\n",
       "    0.75,\n",
       "    0.85311328125,\n",
       "    0.9728964856470526,\n",
       "    0.8247670895614376],\n",
       "   'episode_lengths': [9, 5, 3, 4, 1, 1, 1, 3, 5, 2]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.3457967337075766,\n",
       "   'mean_inference_ms': 1.0051046098981584,\n",
       "   'mean_action_processing_ms': 0.10796646019081016,\n",
       "   'mean_env_wait_ms': 0.09344769762707995,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'off_policy_estimator': {},\n",
       "  'timesteps_this_iter': 0}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1715786)\u001b[0m 2022-02-22 20:04:14,214\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-02-22 20:04:14,398\tWARNING deprecation.py:45 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1715785)\u001b[0m 2022-02-22 20:04:14,373\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "2022-02-22 20:04:14,510\tINFO trainable.py:472 -- Restored on 172.18.0.5 from checkpoint: /home/cboettig/ray_results/PPOTrainer_fishing-v1_2022-02-22_20-03-03ql6e0onn/checkpoint_000004/checkpoint-4\n",
      "2022-02-22 20:04:14,513\tINFO trainable.py:480 -- Current state after restoring: {'_iteration': 4, '_timesteps_total': 16000, '_time_total': 60.387168884277344, '_episodes_total': 8350}\n"
     ]
    }
   ],
   "source": [
    "model = agents.ppo.PPOTrainer(config)\n",
    "# Path will be different\n",
    "model.restore(checkpoint)\n",
    "\n",
    "## Or manually give the path\n",
    "##model.restore(\"/home/cboettig/ray_results/PPOTrainer_fishing-v1_2022-02-22_19-55-51cej434z4/checkpoint_000004/checkpoint-4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1715787)\u001b[0m 2022-02-22 20:04:14,990\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1715788)\u001b[0m 2022-02-22 20:04:15,689\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1716463)\u001b[0m 2022-02-22 20:04:18,809\tWARNING deprecation.py:45 -- DeprecationWarning: `rllib.env.remote_vector_env.RemoteVectorEnv` has been deprecated. Use `ray.rllib.env.remote_base_env.RemoteBaseEnv` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': 1.2160595664436524,\n",
       "  'episode_reward_min': 0.75,\n",
       "  'episode_reward_mean': 0.8503476138371683,\n",
       "  'episode_len_mean': 2.7,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 10,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [0.75,\n",
       "    0.80625,\n",
       "    0.8630928908613323,\n",
       "    0.75,\n",
       "    1.2160595664436524,\n",
       "    0.75,\n",
       "    0.75,\n",
       "    0.9234481767736002,\n",
       "    0.8649533402662901,\n",
       "    0.8296721640268085],\n",
       "   'episode_lengths': [1, 2, 3, 1, 8, 1, 1, 4, 3, 3]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.36495072501046316,\n",
       "   'mean_inference_ms': 0.9745955467224121,\n",
       "   'mean_action_processing_ms': 0.09769201278686523,\n",
       "   'mean_env_wait_ms': 0.08575405393327985,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'off_policy_estimator': {},\n",
       "  'timesteps_this_iter': 0}}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Evaluate the trained Trainer (and render each timestep to the shell's output).\n",
    "model.evaluate()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
