{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#!{sys.executable} -m pip install torch gym_conservation \"ray[rllib, tune]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cboettig/.local/share/virtualenvs/rl-ray-demo-y9hdSGSh/lib/python3.7/site-packages/ale_py/roms/utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_climate\n",
    "import gym_conservation\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21.0, 0.0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training\n",
    "#env = make_vec_env(\"ays-v0\", n_envs=4)\n",
    "#model = A2C(\"MlpPolicy\", env, verbose=1,tensorboard_log=\"/var/log/tensorboard/carl\")\n",
    "#model.learn(total_timesteps=10)\n",
    "#model.save(\"ays-v0-A2C\")\n",
    "\n",
    "# Evaluation\n",
    "model = A2C.load(\"ays-v0-A2C\")\n",
    "eval_env = Monitor(gym.make(\"ays-v0\"))\n",
    "score = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib import agents\n",
    "import torch\n",
    "\n",
    "import gym_conservation\n",
    "import gym_fishing\n",
    "\n",
    "## rllib ignores gym registered names, need to register manually:\n",
    "## note these envs were not written to take a single parameter dictionary (\"config\")\n",
    "tune.register_env(\"conservation-v6\", lambda config: gym_conservation.envs.NonStationaryV6())\n",
    "tune.register_env(\"conservation-v5\", lambda config: gym_conservation.envs.NonStationaryV5())\n",
    "tune.register_env(\"fishing-v0\", lambda config: gym_fishing.envs.FishingEnv())\n",
    "tune.register_env(\"fishing-v1\", lambda config: gym_fishing.envs.FishingCtsEnv())\n",
    "\n",
    "os.environ[\"RLLIB_NUM_GPUS\"] = str(torch.cuda.device_count())\n",
    "os.environ.get(\"RLLIB_NUM_GPUS\")\n",
    "\n",
    "\n",
    "os.environ['RAY_OBJECT_STORE_ALLOW_SLOW_STORAGE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 22:38:28,262\tINFO experiment_analysis.py:695 -- No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Read in an ExperimentAnalysis from a json state file\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "#results_path = \"~/ray_results/ARS/\"\n",
    "results_path = \"~/ray_results/TD3/\"\n",
    "\n",
    "analysis = tune.ExperimentAnalysis(experiment_checkpoint_path=results_path)\n",
    "\n",
    "# status = check_learning_achieved(analysis, 150) # TD3 doesn't meet goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'episode_reward_max': 73.61594719906185,\n",
       " 'episode_reward_min': 32.51658480751109,\n",
       " 'episode_reward_mean': 44.41801731162805,\n",
       " 'episode_len_mean': 497.92,\n",
       " 'episode_media': {},\n",
       " 'episodes_this_iter': 2,\n",
       " 'policy_reward_min': {},\n",
       " 'policy_reward_max': {},\n",
       " 'policy_reward_mean': {},\n",
       " 'custom_metrics': {},\n",
       " 'hist_stats': {'episode_reward': [51.71604481725184,\n",
       "   46.21605940677299,\n",
       "   54.88445713546756,\n",
       "   41.117737699487066,\n",
       "   53.04154396479634,\n",
       "   40.296141160220074,\n",
       "   39.71060423141501,\n",
       "   38.6883339421757,\n",
       "   37.337722913348024,\n",
       "   39.869549477468325,\n",
       "   42.45428257420593,\n",
       "   44.300189259634166,\n",
       "   59.18940100120667,\n",
       "   47.355386740495376,\n",
       "   38.578367892571954,\n",
       "   49.55682233591693,\n",
       "   42.15501828401235,\n",
       "   58.9203660323364,\n",
       "   35.222093950334205,\n",
       "   36.80933707249204,\n",
       "   60.05479419877589,\n",
       "   44.36395543100008,\n",
       "   35.20693692672356,\n",
       "   40.3374343870914,\n",
       "   43.23732838303333,\n",
       "   42.1611176160673,\n",
       "   40.54241242572631,\n",
       "   41.25146119890267,\n",
       "   45.2751185797469,\n",
       "   39.259403762923505,\n",
       "   45.723517718548244,\n",
       "   36.01248523951964,\n",
       "   50.814962656031284,\n",
       "   55.314563017971885,\n",
       "   43.3402809006748,\n",
       "   55.14712363656579,\n",
       "   39.304632685201355,\n",
       "   44.75271347520729,\n",
       "   46.168251921136935,\n",
       "   52.884292926543324,\n",
       "   49.64124150840996,\n",
       "   45.64392111516273,\n",
       "   38.20005545797745,\n",
       "   43.315572281899584,\n",
       "   39.21001433542746,\n",
       "   52.25693147322863,\n",
       "   43.005919561623145,\n",
       "   40.14828345262799,\n",
       "   40.06299548190669,\n",
       "   32.761570731122376,\n",
       "   43.70641977795586,\n",
       "   35.36413269992384,\n",
       "   51.65718501142995,\n",
       "   38.665324477320965,\n",
       "   45.38364739104628,\n",
       "   45.742321460628894,\n",
       "   34.47362532795193,\n",
       "   36.868602598255656,\n",
       "   48.10563337701563,\n",
       "   73.61594719906185,\n",
       "   45.29119527768675,\n",
       "   39.26082358728591,\n",
       "   44.4471535209905,\n",
       "   51.63988574301314,\n",
       "   42.56745455139347,\n",
       "   39.089316663950115,\n",
       "   35.42887647607158,\n",
       "   55.73307569998499,\n",
       "   49.146120696167095,\n",
       "   35.139979267993205,\n",
       "   59.672075872765625,\n",
       "   60.24850567735206,\n",
       "   43.35717777225543,\n",
       "   37.78125992550486,\n",
       "   51.93543298360144,\n",
       "   40.68092035155866,\n",
       "   36.9930354947637,\n",
       "   48.59552935938682,\n",
       "   39.92620684495022,\n",
       "   44.25472749963907,\n",
       "   42.0183063096884,\n",
       "   39.7339563635135,\n",
       "   47.49634878741686,\n",
       "   41.84985537436966,\n",
       "   39.32002231740503,\n",
       "   46.68173102157467,\n",
       "   36.3970503894018,\n",
       "   45.49758998663298,\n",
       "   62.10449248858107,\n",
       "   49.179463038925185,\n",
       "   45.395485397632655,\n",
       "   43.31726673525428,\n",
       "   50.867272233524716,\n",
       "   46.10780994757157,\n",
       "   43.882023788613786,\n",
       "   39.396959010917,\n",
       "   37.107602192085466,\n",
       "   38.696390248937796,\n",
       "   32.51658480751109,\n",
       "   36.64517575795451],\n",
       "  'episode_lengths': [501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   444,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   499,\n",
       "   501,\n",
       "   501,\n",
       "   453,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   498,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   442,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   491,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   498,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   422,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   501,\n",
       "   454,\n",
       "   501]},\n",
       " 'sampler_perf': {'mean_raw_obs_processing_ms': 0.6345107699556677,\n",
       "  'mean_inference_ms': 1.5897322949760846,\n",
       "  'mean_action_processing_ms': 0.17497093545612155,\n",
       "  'mean_env_wait_ms': 0.3773807937238582,\n",
       "  'mean_env_render_ms': 0.0},\n",
       " 'off_policy_estimator': {},\n",
       " 'num_healthy_workers': 2,\n",
       " 'timesteps_total': 300000,\n",
       " 'timesteps_this_iter': 100,\n",
       " 'agent_timesteps_total': 300000,\n",
       " 'timers': {'load_time_ms': 1.228,\n",
       "  'load_throughput': 81411.18,\n",
       "  'learn_time_ms': 12.261,\n",
       "  'learn_throughput': 8155.777,\n",
       "  'update_time_ms': 2.874},\n",
       " 'info': {'learner': {'default_policy': {'learner_stats': {'actor_loss': -4.693178653717041,\n",
       "     'critic_loss': 0.513960412699762,\n",
       "     'mean_q': 4.631434917449951,\n",
       "     'max_q': 15.22116470336914,\n",
       "     'min_q': 2.836329460144043},\n",
       "    'td_error': array([-6.56080246e-02, -1.01952791e-01,  8.78794193e-02, -4.65621948e-02,\n",
       "           -5.78393936e-02, -1.15657043e+00, -2.90234089e-02, -2.04553604e-02,\n",
       "            2.17037201e-02, -1.23975754e-01, -9.91837978e-02, -2.48524570e+00,\n",
       "           -5.54990768e-02, -3.56500149e-02, -2.92405128e-01, -7.35521317e-02,\n",
       "           -1.08557940e-01, -6.15606308e-02,  3.09314632e+00, -6.98485374e-02,\n",
       "           -6.19838238e-02,  9.57655907e-03, -3.81681919e-02, -3.97212505e-02,\n",
       "            3.61356735e-02,  4.21583652e-02, -1.07532024e-01, -6.43517971e-02,\n",
       "           -8.27614784e-01,  3.00509930e-02, -5.50591946e-02,  3.98864508e-01,\n",
       "            4.00815010e-02, -3.13105583e-02, -1.85107231e-01, -6.51414394e-02,\n",
       "           -5.00860214e-02, -8.21583271e-02, -3.51651478e+00, -7.63578415e-02,\n",
       "           -5.33783436e-02, -4.70316410e-02,  4.51612473e-02,  6.26301765e-02,\n",
       "            4.04506445e-01, -5.97436428e-02,  2.41658211e-01,  2.06644535e-02,\n",
       "           -6.53536320e-02,  9.18574095e-01, -9.50157642e-02, -2.72111893e-02,\n",
       "           -6.37280941e-02, -7.67467022e-02, -1.13639832e-02,  2.67910957e-03,\n",
       "            3.44857216e-01, -9.63306427e-02,  1.40905380e-03,  3.65040302e-02,\n",
       "           -6.14755154e-02, -1.42643452e-02, -8.17630291e-02,  1.88713789e-01,\n",
       "           -4.47909832e-02,  2.06886959e+00, -7.33754635e-02, -2.54871845e-02,\n",
       "           -1.03045464e-01,  1.95151091e-01, -1.44039869e-01, -4.84600067e-02,\n",
       "           -8.69672298e-02,  3.74407530e-01,  2.19895840e-02, -5.61423302e-01,\n",
       "            1.94890213e+00, -6.88905716e-02, -1.09930038e-01, -1.32713318e-02,\n",
       "           -9.31556225e-02, -9.43636894e-02, -6.88149452e-01, -4.66330051e-02,\n",
       "           -8.49523544e-02, -6.23767853e-01,  2.59589338e+00, -5.02810478e-02,\n",
       "           -7.10873604e-02, -2.54962444e-02, -6.32598400e-02, -1.94718266e+00,\n",
       "           -8.55545998e-02,  2.04541683e-01, -7.88674355e-02,  1.55334473e-01,\n",
       "           -6.78772926e-02, -1.00553274e-01, -7.19583035e-02, -8.54113102e-02],\n",
       "          dtype=float32),\n",
       "    'mean_td_error': -0.02804192528128624}},\n",
       "  'num_steps_sampled': 300000,\n",
       "  'num_agent_steps_sampled': 300000,\n",
       "  'num_steps_trained': 14500100,\n",
       "  'num_steps_trained_this_iter': 100,\n",
       "  'num_agent_steps_trained': 14500100,\n",
       "  'last_target_update_ts': 300000,\n",
       "  'num_target_updates': 145001},\n",
       " 'done': True,\n",
       " 'episodes_total': 600,\n",
       " 'training_iteration': 291,\n",
       " 'trial_id': '094ac_00000',\n",
       " 'experiment_id': 'a6448bda51104ba383401ab42e6773e4',\n",
       " 'date': '2022-02-16_17-23-45',\n",
       " 'timestamp': 1645032225,\n",
       " 'time_this_iter_s': 22.660517930984497,\n",
       " 'time_total_s': 6476.958378791809,\n",
       " 'pid': 1839102,\n",
       " 'hostname': 'f3c1977db4f9',\n",
       " 'node_ip': '172.18.0.5',\n",
       " 'config': {'num_workers': 2,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'create_env_on_driver': False,\n",
       "  'rollout_fragment_length': 1,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'gamma': 0.99,\n",
       "  'lr': 0.0001,\n",
       "  'train_batch_size': 100,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   'fcnet_hiddens': [256, 256],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': False,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'framestack': True,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1},\n",
       "  'optimizer': {},\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': 'conservation-v6',\n",
       "  'observation_space': None,\n",
       "  'action_space': None,\n",
       "  'env_config': {},\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'clip_rewards': False,\n",
       "  'normalize_actions': True,\n",
       "  'clip_actions': False,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'torch',\n",
       "  'eager_tracing': False,\n",
       "  'eager_max_retraces': 20,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'GaussianNoise',\n",
       "   'random_timesteps': 10000,\n",
       "   'stddev': 0.1,\n",
       "   'initial_scale': 1.0,\n",
       "   'final_scale': 1.0,\n",
       "   'scale_timesteps': 1},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_duration': 10,\n",
       "  'evaluation_duration_unit': 'episodes',\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {'num_workers': 2,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'create_env_on_driver': False,\n",
       "   'rollout_fragment_length': 1,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'gamma': 0.99,\n",
       "   'lr': 0.0001,\n",
       "   'train_batch_size': 100,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    '_disable_preprocessor_api': False,\n",
       "    'fcnet_hiddens': [256, 256],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': False,\n",
       "    'vf_share_layers': True,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'framestack': True,\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': None,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1},\n",
       "   'optimizer': {},\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'env': 'conservation-v6',\n",
       "   'observation_space': None,\n",
       "   'action_space': None,\n",
       "   'env_config': {},\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'record_env': False,\n",
       "   'clip_rewards': False,\n",
       "   'normalize_actions': True,\n",
       "   'clip_actions': False,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'log_level': 'WARN',\n",
       "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "   'ignore_worker_failures': False,\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'framework': 'torch',\n",
       "   'eager_tracing': False,\n",
       "   'eager_max_retraces': 20,\n",
       "   'explore': False,\n",
       "   'exploration_config': {'type': 'GaussianNoise',\n",
       "    'random_timesteps': 10000,\n",
       "    'stddev': 0.1,\n",
       "    'initial_scale': 1.0,\n",
       "    'final_scale': 1.0,\n",
       "    'scale_timesteps': 1},\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_duration': 10,\n",
       "   'evaluation_duration_unit': 'episodes',\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'in_evaluation': False,\n",
       "   'evaluation_config': {'explore': False},\n",
       "   'evaluation_num_workers': 0,\n",
       "   'custom_eval_function': None,\n",
       "   'always_attach_evaluation_results': False,\n",
       "   'sample_async': False,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'compress_observations': False,\n",
       "   'metrics_episode_collection_timeout_s': 180,\n",
       "   'metrics_num_episodes_for_smoothing': 100,\n",
       "   'min_time_s_per_reporting': 1,\n",
       "   'min_train_timesteps_per_reporting': None,\n",
       "   'min_sample_timesteps_per_reporting': 1000,\n",
       "   'seed': None,\n",
       "   'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 1,\n",
       "   '_fake_gpus': False,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'num_cpus_for_driver': 1,\n",
       "   'placement_strategy': 'PACK',\n",
       "   'input': 'sampler',\n",
       "   'input_config': {},\n",
       "   'actions_in_input_normalized': False,\n",
       "   'input_evaluation': ['is', 'wis'],\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'multiagent': {'policies': {'default_policy': [ray.rllib.policy.policy_template.DDPGTorchPolicy,\n",
       "      None,\n",
       "      None,\n",
       "      {}]},\n",
       "    'policy_map_capacity': 100,\n",
       "    'policy_map_cache': None,\n",
       "    'policy_mapping_fn': None,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'replay_mode': 'independent',\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'logger_config': None,\n",
       "   '_tf_policy_handles_more_than_one_loss': False,\n",
       "   '_disable_preprocessor_api': False,\n",
       "   '_disable_action_flattening': False,\n",
       "   '_disable_execution_plan_api': False,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'evaluation_num_episodes': -1,\n",
       "   'metrics_smoothing_episodes': -1,\n",
       "   'timesteps_per_iteration': 1000,\n",
       "   'min_iter_time_s': 1,\n",
       "   'collect_metrics_timeout': -1,\n",
       "   'twin_q': True,\n",
       "   'policy_delay': 2,\n",
       "   'smooth_target_policy': True,\n",
       "   'target_noise': 0.2,\n",
       "   'target_noise_clip': 0.5,\n",
       "   'use_state_preprocessor': False,\n",
       "   'actor_hiddens': [400, 300],\n",
       "   'actor_hidden_activation': 'relu',\n",
       "   'critic_hiddens': [400, 300],\n",
       "   'critic_hidden_activation': 'relu',\n",
       "   'n_step': 1,\n",
       "   'buffer_size': 1000000,\n",
       "   'replay_buffer_config': {'type': 'MultiAgentReplayBuffer',\n",
       "    'capacity': 50000},\n",
       "   'store_buffer_in_checkpoints': False,\n",
       "   'prioritized_replay': False,\n",
       "   'prioritized_replay_alpha': 0.6,\n",
       "   'prioritized_replay_beta': 0.4,\n",
       "   'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "   'final_prioritized_replay_beta': 0.4,\n",
       "   'prioritized_replay_eps': 1e-06,\n",
       "   'training_intensity': None,\n",
       "   'critic_lr': 0.001,\n",
       "   'actor_lr': 0.001,\n",
       "   'target_network_update_freq': 0,\n",
       "   'tau': 0.005,\n",
       "   'use_huber': False,\n",
       "   'huber_threshold': 1.0,\n",
       "   'l2_reg': 0.0,\n",
       "   'grad_clip': None,\n",
       "   'learning_starts': 10000,\n",
       "   'worker_side_prioritization': False},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'always_attach_evaluation_results': False,\n",
       "  'sample_async': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'metrics_episode_collection_timeout_s': 180,\n",
       "  'metrics_num_episodes_for_smoothing': 100,\n",
       "  'min_time_s_per_reporting': 1,\n",
       "  'min_train_timesteps_per_reporting': None,\n",
       "  'min_sample_timesteps_per_reporting': 1000,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 1,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'input': 'sampler',\n",
       "  'input_config': {},\n",
       "  'actions_in_input_normalized': False,\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {'default_policy': [ray.rllib.policy.policy_template.DDPGTorchPolicy,\n",
       "     None,\n",
       "     None,\n",
       "     {}]},\n",
       "   'policy_map_capacity': 100,\n",
       "   'policy_map_cache': None,\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'logger_config': None,\n",
       "  '_tf_policy_handles_more_than_one_loss': False,\n",
       "  '_disable_preprocessor_api': False,\n",
       "  '_disable_action_flattening': False,\n",
       "  '_disable_execution_plan_api': False,\n",
       "  'simple_optimizer': False,\n",
       "  'monitor': -1,\n",
       "  'evaluation_num_episodes': -1,\n",
       "  'metrics_smoothing_episodes': -1,\n",
       "  'timesteps_per_iteration': 1000,\n",
       "  'min_iter_time_s': 1,\n",
       "  'collect_metrics_timeout': -1,\n",
       "  'twin_q': True,\n",
       "  'policy_delay': 2,\n",
       "  'smooth_target_policy': True,\n",
       "  'target_noise': 0.2,\n",
       "  'target_noise_clip': 0.5,\n",
       "  'use_state_preprocessor': False,\n",
       "  'actor_hiddens': [400, 300],\n",
       "  'actor_hidden_activation': 'relu',\n",
       "  'critic_hiddens': [400, 300],\n",
       "  'critic_hidden_activation': 'relu',\n",
       "  'n_step': 1,\n",
       "  'buffer_size': 1000000,\n",
       "  'replay_buffer_config': {'type': 'MultiAgentReplayBuffer',\n",
       "   'capacity': 50000},\n",
       "  'store_buffer_in_checkpoints': False,\n",
       "  'prioritized_replay': False,\n",
       "  'prioritized_replay_alpha': 0.6,\n",
       "  'prioritized_replay_beta': 0.4,\n",
       "  'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "  'final_prioritized_replay_beta': 0.4,\n",
       "  'prioritized_replay_eps': 1e-06,\n",
       "  'training_intensity': None,\n",
       "  'critic_lr': 0.001,\n",
       "  'actor_lr': 0.001,\n",
       "  'target_network_update_freq': 0,\n",
       "  'tau': 0.005,\n",
       "  'use_huber': False,\n",
       "  'huber_threshold': 1.0,\n",
       "  'l2_reg': 0.0,\n",
       "  'grad_clip': None,\n",
       "  'learning_starts': 10000,\n",
       "  'worker_side_prioritization': False},\n",
       " 'time_since_restore': 6476.958378791809,\n",
       " 'timesteps_since_restore': 29100,\n",
       " 'iterations_since_restore': 291,\n",
       " 'perf': {'cpu_util_percent': 14.479166666666666,\n",
       "  'ram_util_percent': 36.7,\n",
       "  'gpu_util_percent0': 0.03666666666666667,\n",
       "  'vram_util_percent0': 0.1436270209299411},\n",
       " 'experiment_tag': '0'}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config = analysis.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\").last_result[\"config\"]\n",
    "\n",
    "\n",
    "#analysis.dataframe(metric=\"episode_reward_mean\", mode=\"max\")\n",
    "analysis.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\").last_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the hyper-parameter config\n",
    "config = analysis.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\").last_result[\"config\"]\n",
    "config.pop(\"in_evaluation\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 19:54:26,934\tWARNING deprecation.py:46 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "2022-02-17 19:54:26,936\tWARNING deprecation.py:46 -- DeprecationWarning: `config['buffer_size']` has been deprecated. Use `config['replay_buffer_config']['capacity']` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "## Okay, now we can initialize a model \n",
    "model = agents.ddpg.TD3Trainer(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/cboettig/ray_results/TD3/.tune_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3051/320364916.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/cboettig/ray_results/TD3/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/share/virtualenvs/rl-ray-demo-y9hdSGSh/lib/python3.7/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".tune_metadata\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m             \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experiment_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"experiment_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/cboettig/ray_results/TD3/.tune_metadata'"
     ]
    }
   ],
   "source": [
    "model.restore(\"/home/cboettig/ray_results/TD3/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "params = \"/home/cboettig/ray_results/TD3/TD3_conservation-v6_094ac_00000_0_2022-02-16_15-35-14/params.json\"\n",
    "with open(params) as json_file:\n",
    "    data = json.load(json_file)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'episode_reward_max': 1.2274416977413574,\n",
       "  'episode_reward_min': 0.75,\n",
       "  'episode_reward_mean': 0.8675837622445943,\n",
       "  'episode_len_mean': 3.8,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 10,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [0.80625,\n",
       "    0.80625,\n",
       "    1.2274416977413574,\n",
       "    0.75,\n",
       "    0.75,\n",
       "    1.0980312748711576,\n",
       "    0.80625,\n",
       "    0.80625,\n",
       "    0.8191146498334291,\n",
       "    0.80625],\n",
       "   'episode_lengths': [2, 2, 9, 1, 1, 14, 2, 2, 3, 2]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.3598704934120178,\n",
       "   'mean_inference_ms': 1.170910894870758,\n",
       "   'mean_action_processing_ms': 0.11758009592692058,\n",
       "   'mean_env_wait_ms': 0.10723620653152466,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'off_policy_estimator': {},\n",
       "  'timesteps_this_iter': 0}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Evaluate the trained Trainer (and render each timestep to the shell's output).\n",
    "trainer.evaluate()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
